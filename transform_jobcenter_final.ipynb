{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc7a2c6",
   "metadata": {},
   "source": [
    "# Step 1: Research & Data Source Discovery\n",
    "\n",
    "## 1.1 Environment Setup and Dependencies\n",
    "\n",
    "We fetch the data from OpenStreetMap. We use the original OSM ID (osmid) as our primary identifier and calculate the exact center point (latitude and longitude) for each location.\n",
    "\n",
    "* **Primary Source: OpenStreetMap (OSM)**: Used to extract the spatial location of employment agencies.\n",
    "\n",
    "\n",
    "## 1.2 Data and Boundary Configuration\n",
    "\n",
    "The project focuses exclusively on data within the **Berlin, Germany** boundary.\n",
    "\n",
    "* **Spatial Integrity Plan**: Data will be joined to the **Local Reference System (LOR) boundaries** to derive the mandatory `district_id` and `neighborhood_id` for final database compliance.\n",
    "\n",
    "\n",
    "* **Import all necessary libraries**\n",
    " * Imports used across this notebook: pandas, geopandas, osmnx, geopy, sqlalchemy/psycopg2, shapely, hashlib, json, os, warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55e2ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in /opt/anaconda3/lib/python3.13/site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /opt/anaconda3/lib/python3.13/site-packages (from geopy) (2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sqlalchemy in /opt/anaconda3/lib/python3.13/site-packages (2.0.39)\n",
      "Requirement already satisfied: psycopg2-binary in /opt/anaconda3/lib/python3.13/site-packages (2.9.11)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from sqlalchemy) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import os \n",
    "import json\n",
    "%pip install geopy\n",
    "%pip install sqlalchemy psycopg2-binary\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "from geopy.geocoders import Nominatim\n",
    "from time import sleep\n",
    "import hashlib\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7aea4c",
   "metadata": {},
   "source": [
    "# 1.1 CONFIGURATION \n",
    "- Using the specific paths and tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3555ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded.\n",
      "Configuration set for Berlin, Germany with OSM tags: {'office': 'employment_agency'}\n"
     ]
    }
   ],
   "source": [
    "PLACE_NAME = \"Berlin, Germany\"\n",
    "OSM_TAGS = {\"office\": \"employment_agency\"}\n",
    "\n",
    "# Update LOR_PATH to match the exact filename of the GeoJSON you uploaded\n",
    "#LOR_PATH = \"lor_ortsteile (1).geojson\" \n",
    "#OUTPUT_PATH = \"output/jobcenters_berlin.csv\"\n",
    "\n",
    "print(\"Libraries loaded.\")\n",
    "print(f\"Configuration set for {PLACE_NAME} with OSM tags: {OSM_TAGS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65013f68",
   "metadata": {},
   "source": [
    "# 1.2 LIVE DATA EXTRACTION (OSM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff1ebfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching live data from OpenStreetMap (Overpass API)\n",
      "Success! Retrieved 65 features.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching live data from OpenStreetMap (Overpass API)\")\n",
    "try:\n",
    "    # Fetch data and ensure the coordinate system is standard WGS84 (EPSG:4326)\n",
    "    jobcenter_data_raw = ox.features_from_place(PLACE_NAME, OSM_TAGS)\n",
    "    jobcenter_data_raw = gpd.GeoDataFrame(\n",
    "        jobcenter_data_raw,\n",
    "        geometry=\"geometry\",\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    print(f\"Success! Retrieved {len(jobcenter_data_raw)} features.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"OSM extraction failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c5bd87",
   "metadata": {},
   "source": [
    "# 1.3 MANDATORY DATA CLEANING \n",
    "- Explicitly check and report on null values in mandatory columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2de87156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic Check: Nulls in Critical Columns\n",
      "Missing values in critical columns\n",
      "name        2\n",
      "geometry    0\n",
      "dtype: int64\n",
      "Mandatory Drop Removed 2 rows due to missing name/geometry\n"
     ]
    }
   ],
   "source": [
    "print(\"Diagnostic Check: Nulls in Critical Columns\")\n",
    "null_counts = jobcenter_data_raw[['name', 'geometry']].isnull().sum()\n",
    "print(\"Missing values in critical columns\")\n",
    "print(null_counts)\n",
    "\n",
    "# Drop rows missing 'name' or 'geometry' to enforce database NOT NULL compliance\n",
    "initial_count = len(jobcenter_data_raw)\n",
    "jobcenter_enriched = jobcenter_data_raw.dropna(subset=[\"name\", \"geometry\"]).copy()\n",
    "\n",
    "dropped_count = initial_count - len(jobcenter_enriched)\n",
    "print(f\"Mandatory Drop Removed {dropped_count} rows due to missing name/geometry\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9eac3",
   "metadata": {},
   "source": [
    "# 1.4 COORDINATE PREPARATION \n",
    "- Extract centroids to handle both 'Point' and 'Polygon' features safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66ca79ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 Complete \n",
      "                                               name   latitude  longitude\n",
      "element id                                                               \n",
      "node    275368512   Jobcenter Mitte am Leopoldplatz  52.546772  13.356516\n",
      "        1211913324           Arbeitsagentur Spandau  52.533775  13.186554\n",
      "        1340158173        Jobcenter Berlin Neuk√∂lln  52.478975  13.427887\n",
      "        1450906609               Agentur f√ºr Arbeit  52.578452  13.308718\n",
      "        2277566662               Agentur f√ºr Arbeit  52.456592  13.411478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w5/hrcdt9v17ws_d71g1h0pcwjc0000gn/T/ipykernel_20380/340869887.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  jobcenter_enriched['latitude'] = jobcenter_enriched.geometry.centroid.y\n",
      "/var/folders/w5/hrcdt9v17ws_d71g1h0pcwjc0000gn/T/ipykernel_20380/340869887.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  jobcenter_enriched['longitude'] = jobcenter_enriched.geometry.centroid.x\n"
     ]
    }
   ],
   "source": [
    "jobcenter_enriched['latitude'] = jobcenter_enriched.geometry.centroid.y\n",
    "jobcenter_enriched['longitude'] = jobcenter_enriched.geometry.centroid.x\n",
    "\n",
    "print(\"Step 1 Complete \")\n",
    "print(jobcenter_enriched[['name', 'latitude', 'longitude']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e57c48",
   "metadata": {},
   "source": [
    "# Step 2: Cleanup & Removing Redundancy\n",
    "- Explanation: Here i drop city and country because they are redundant for a Berlin project. Also removed contact-website and operator-type to keep the schema lean.\n",
    "  Why drop contact \"website\"\n",
    "\n",
    "- Maintenance: External URLs like websites change frequently. If its included in the primary table now, the data becomes \"stale\" very quickly.\n",
    "\n",
    "- Scope: The current goal is to map the job centers to the Berlin District LOR system. Extra information like websites or phone numbers can be added in a later \"enrichment\"         task once the primary table structure is approved.\n",
    "\n",
    "- Additionally: The operator:type column is a classification tag in OpenStreetMap. It tells the database who runs the facility. In the context of Berlin Job Centers, this usually   indicates public. \n",
    "\n",
    "- The center is a government-run entity (e.g., the Bundesagentur f√ºr Arbeit or local municipal government). Most Job Centers fall into this category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef9653",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 INITIAL CLEANUP Rename and prepare coordinates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92e7d56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w5/hrcdt9v17ws_d71g1h0pcwjc0000gn/T/ipykernel_20380/2949301454.py:11: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroids = jobcenter_clean.geometry.centroid\n"
     ]
    }
   ],
   "source": [
    "#  Prefer the cleaned/enriched frame from earlier fall back to the raw OSM frame\n",
    "if 'jobcenter_enriched' in globals():\n",
    "    jobcenter_clean = jobcenter_enriched.copy()\n",
    "elif 'jobcenter_data_raw' in globals():\n",
    "    jobcenter_clean = jobcenter_data_raw.copy()\n",
    "else:\n",
    "    raise NameError(\"Expected 'jobcenter_enriched' or 'jobcenter_data_raw' to be defined.\")\n",
    "jobcenter_clean = jobcenter_clean.rename(columns={'name': 'center_name'})\n",
    "\n",
    "# Calculate centroids to ensure we have lat/lon for both Points and Polygons\n",
    "centroids = jobcenter_clean.geometry.centroid\n",
    "jobcenter_clean['latitude'] = centroids.y\n",
    "jobcenter_clean['longitude'] = centroids.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b5c72",
   "metadata": {},
   "source": [
    "### 2.2  BUILD ADDRESS FROM COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6cc0a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using fillna('') to avoid \"NaN\" appearing in the text strings\n",
    "jobcenter_clean['address'] = (\n",
    "    jobcenter_clean['addr:street'].fillna('') + ' ' + \n",
    "    jobcenter_clean['addr:housenumber'].fillna('')\n",
    ").str.strip()\n",
    "\n",
    "# Add house name in brackets if it exists (e.g., \"Jobcenter Mitte\")\n",
    "mask_housename = jobcenter_clean['addr:housename'].notna()\n",
    "jobcenter_clean.loc[mask_housename, 'address'] = (\n",
    "    jobcenter_clean['address'] + ' (' + jobcenter_clean['addr:housename'] + ')'\n",
    ").str.strip()\n",
    "# Map the postal code from OSM\n",
    "jobcenter_clean['postal_code'] = jobcenter_clean['addr:postcode']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3f602",
   "metadata": {},
   "source": [
    "### 2.3 NOMINATIM FALLBACK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a762edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 14 rows needing Nominatim enrichment. Starting fallback...\n",
      "Verification of Enriched Data\n",
      "                                        center_name  \\\n",
      "element id                                            \n",
      "node    275368512   Jobcenter Mitte am Leopoldplatz   \n",
      "        1211913324           Arbeitsagentur Spandau   \n",
      "        1340158173        Jobcenter Berlin Neuk√∂lln   \n",
      "        1450906609               Agentur f√ºr Arbeit   \n",
      "        2277566662               Agentur f√ºr Arbeit   \n",
      "\n",
      "                                                              address  \\\n",
      "element id                                                              \n",
      "node    275368512   M√ºllerstra√üe 147 (Jobcenter Mitte am Leopoldpl...   \n",
      "        1211913324                           Brunsb√ºtteler Damm 75-77   \n",
      "        1340158173                                  Mainzer Stra√üe 27   \n",
      "        1450906609                                   Innungsstra√üe 40   \n",
      "        2277566662  Agentur f√ºr Arbeit, 43-44, Gottlieb-Dunkel-Str...   \n",
      "\n",
      "                   postal_code  \n",
      "element id                      \n",
      "node    275368512        13353  \n",
      "        1211913324       13581  \n",
      "        1340158173       12053  \n",
      "        1450906609       13509  \n",
      "        2277566662       12099  \n"
     ]
    }
   ],
   "source": [
    "geolocator = Nominatim(user_agent=\"berlin_jobcenter_locator\")\n",
    "\n",
    "def get_nominatim_data(lat, lon):\n",
    "    \"\"\"Retrieves both address and postcode from Nominatim\"\"\"\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), exactly_one=True, language='de')\n",
    "        sleep(1) # Crucial: Respect Nominatim's 1-second rate limit\n",
    "        if location:\n",
    "            address_text = location.address\n",
    "            postcode = location.raw.get('address', {}).get('postcode')\n",
    "            return address_text, postcode\n",
    "        return None, None\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "# Find rows where address is still empty OR postal_code is NaN\n",
    "mask_missing = (jobcenter_clean['address'] == \"\") | (jobcenter_clean['postal_code'].isna())\n",
    "\n",
    "if mask_missing.any():\n",
    "    print(f\"üîç Found {mask_missing.sum()} rows needing Nominatim enrichment. Starting fallback...\")\n",
    "    \n",
    "    # We apply the function to fill both columns at once\n",
    "    results = jobcenter_clean[mask_missing].apply(\n",
    "        lambda row: get_nominatim_data(row['latitude'], row['longitude']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Extract the results back into the dataframe\n",
    "    jobcenter_clean.loc[mask_missing, 'address'] = [r[0] for r in results]\n",
    "    jobcenter_clean.loc[mask_missing, 'postal_code'] = [r[1] for r in results]\n",
    "else:\n",
    "    print(\" All addresses and postal codes were successfully built from existing data!\")\n",
    "\n",
    "print(\"Verification of Enriched Data\")\n",
    "print(jobcenter_clean[['center_name', 'address', 'postal_code']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84239880",
   "metadata": {},
   "source": [
    "## Step 3: Spatial Mapping (District Join)\n",
    "\n",
    "- Explanation - Load the official Berlin district file and  use a Spatial Join to see which district polygon each job center point \"falls into.\" This gives us the neighborhood and  district names automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e942560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOR_PATH = \"lor_ortsteile.geojson\"\n",
    "lor_gdf = gpd.read_file(LOR_PATH).to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ffa3fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOR file exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"LOR file exists:\", os.path.exists(LOR_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8698bf",
   "metadata": {},
   "source": [
    "### 3.1 Safe Renaming: Only rename if the old columns still exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d9d13a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join completed successfully (even on a re-run!)\n"
     ]
    }
   ],
   "source": [
    "if \"BEZIRK\" in lor_gdf.columns:\n",
    "    lor_gdf = lor_gdf.rename(columns={\n",
    "        \"BEZIRK\": \"district\",\n",
    "        \"OTEIL\": \"neighborhood\",\n",
    "        \"spatial_name\": \"neighborhood_id\"\n",
    "    })\n",
    "\n",
    "#  Safety check Remove 'index_right' to prevent the SJOIN ValueError\n",
    "if 'index_right' in jobcenter_clean.columns:\n",
    "    jobcenter_clean = jobcenter_clean.drop(columns=['index_right'])\n",
    "\n",
    "#  Spatial Join\n",
    "jobcenter_mapped = gpd.sjoin(\n",
    "    jobcenter_clean.reset_index(drop=True), \n",
    "    lor_gdf[['district', 'neighborhood', 'neighborhood_id', 'geometry']], \n",
    "    how='left', \n",
    "    predicate='within'\n",
    ")\n",
    "\n",
    "print(\"Join completed successfully (even on a re-run!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a96d7",
   "metadata": {},
   "source": [
    "## 4: Stable ID Generation and District Mapping\n",
    "Deterministic Stable ID: A persistent, numeric-only ID is generated using hashlib.sha256. By hashing the geographic centroid, we ensure IDs are unique and immutable, avoiding previous AttributeError issues with different geometry types.\n",
    "\n",
    "Official District Mapping: To comply with the final data pool schema, we map administrative district names to their official 8-digit numeric IDs (e.g., Mitte = 11001001). This ensures the data is ready for SQL relational joins.hment:** The `enrich_data_from_wikidata` function is applied to fill the `operator_name` and `contact_website` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05978d",
   "metadata": {},
   "source": [
    "### 4.1 DEFINITIONS \n",
    "- By using a deterministic hashing algorithm to create stable, 8-10 digit IDs. This ensures that even if we refresh the data, the same job center will always keep the same ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c799134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stable_id(name, lat, lon):\n",
    "    \"\"\"Generates a unique 10-digit ID based on name and coordinates.\"\"\"\n",
    "    input_data = f\"{name}_{lat}_{lon}\".encode('utf-8')\n",
    "    hash_hex = hashlib.sha256(input_data).hexdigest()\n",
    "    return int(hash_hex, 16) % (10**10)\n",
    "\n",
    "district_mapping = {\n",
    "    'Mitte': '11001001', 'Friedrichshain-Kreuzberg': '11002002',\n",
    "    'Pankow': '11003003', 'Charlottenburg-Wilmersdorf': '11004004',\n",
    "    'Spandau': '11005005', 'Steglitz-Zehlendorf': '11006006',\n",
    "    'Tempelhof-Sch√∂neberg': '11007007', 'Neuk√∂lln': '11008008',\n",
    "    'Treptow-K√∂penick': '11009009', 'Marzahn-Hellersdorf': '11010010',\n",
    "    'Lichtenberg': '11011011', 'Reinickendorf': '11012012'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc313606",
   "metadata": {},
   "source": [
    "### 4.2 EXECUTION (The Calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dce70e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating stable IDs...\n",
      "Mapping districts...\n",
      "Step 4 complete. Data is enriched and identified.\n",
      "Mapping district names to official IDs...\n",
      "Final verification of mapped data\n",
      "           id                      center_name              district  \\\n",
      "0  6660665090  Jobcenter Mitte am Leopoldplatz                 Mitte   \n",
      "1  1092468394           Arbeitsagentur Spandau               Spandau   \n",
      "2   730832232        Jobcenter Berlin Neuk√∂lln              Neuk√∂lln   \n",
      "3   246338546               Agentur f√ºr Arbeit         Reinickendorf   \n",
      "4  6239357044               Agentur f√ºr Arbeit  Tempelhof-Sch√∂neberg   \n",
      "\n",
      "  district_id  \n",
      "0    11001001  \n",
      "1    11005005  \n",
      "2    11008008  \n",
      "3    11012012  \n",
      "4    11007007  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w5/hrcdt9v17ws_d71g1h0pcwjc0000gn/T/ipykernel_20380/2223890943.py:2: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  jobcenter_mapped['latitude'] = jobcenter_mapped.geometry.centroid.y\n",
      "/var/folders/w5/hrcdt9v17ws_d71g1h0pcwjc0000gn/T/ipykernel_20380/2223890943.py:3: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  jobcenter_mapped['longitude'] = jobcenter_mapped.geometry.centroid.x\n"
     ]
    }
   ],
   "source": [
    "# Coordinate Prep (Ensuring columns exist)\n",
    "jobcenter_mapped['latitude'] = jobcenter_mapped.geometry.centroid.y\n",
    "jobcenter_mapped['longitude'] = jobcenter_mapped.geometry.centroid.x\n",
    "\n",
    "# Call the Stable ID function\n",
    "print(\"Generating stable IDs...\")\n",
    "jobcenter_mapped['id'] = jobcenter_mapped.apply(\n",
    "    lambda row: generate_stable_id(row['center_name'], row['latitude'], row['longitude']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Call the District mapping\n",
    "print(\"Mapping districts...\")\n",
    "jobcenter_mapped['district_id'] = jobcenter_mapped['district'].map(district_mapping)\n",
    "\n",
    "print(\"Step 4 complete. Data is enriched and identified.\")\n",
    "print(\"Mapping district names to official IDs...\")\n",
    "jobcenter_mapped['district_id'] = jobcenter_mapped['district'].map(district_mapping).astype(str)\n",
    "\n",
    "print(\"Final verification of mapped data\")\n",
    "\n",
    "print(jobcenter_mapped[['id', 'center_name', 'district', 'district_id']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b601d",
   "metadata": {},
   "source": [
    "# 5 Data Standardization and Final Export\n",
    "Schema Compliance: The final dataset is filtered to include only the mandatory 8 columns required for the database pool: id, district_id, center_name, latitude, longitude, neighborhood, district, and neighborhood_id.\n",
    "\n",
    "WKT & Coordinate Prep: Coordinates are extracted from the geometric centroids and formatted as numeric floats, ensuring compatibility with standard SQL spatial types.\n",
    "\n",
    "Stable ID Integration: The deterministic IDs generated in Step 4 are finalized as the primary keys for this dataset.\n",
    "\n",
    "Data Source Attribution: A data_source tag (OSM_LOR) is appended to ensure traceability for future audits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d7fa5",
   "metadata": {},
   "source": [
    "### 5.1 CLEANING & QUALITY CONTROL\n",
    "Data Sanitization - Normalize text by stripping whitespace and converting invisible empty strings into standardized Null values to ensure database integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd9cc8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fresh from your mapped data\n",
    "df_final = jobcenter_mapped.copy()\n",
    "\n",
    "# Fix the ID format Ensure IDs are 10-digit strings without decimals or spaces\n",
    "df_final['id'] = df_final['id'].apply(lambda x: str(int(float(x))).strip() if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "075fe106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean invisible spaces strip whitespace and convert empty text/placeholders to NaN\n",
    "for col in ['center_name', 'address']:\n",
    "    df_final[col] = df_final[col].astype(str).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "468fec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag all variations of \"empty\" as true NaN values\n",
    "df_final = df_final.replace(['', 'None', 'nan', 'NaN', 'nan '], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "adf74bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop incomplete rows to hit the production target\n",
    "df_final = df_final.dropna(subset=['center_name', 'address'], how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44bb2ef",
   "metadata": {},
   "source": [
    "### 5.2 SCHEMA & GEOMETRY SERIALIZATION\n",
    "Serialization for Production - Since SQL databases cannot natively interpret Python geometry objects, converting map coordinates into Well-Known Text (WKT) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ad33801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Backup: Saved 63 records to output/jobcenters_berlin_final.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w5/hrcdt9v17ws_d71g1h0pcwjc0000gn/T/ipykernel_20380/1182810994.py:11: UserWarning: Geometry column does not contain geometry.\n",
      "  df_final['geometry'] = df_final['geometry'].apply(lambda x: x.wkt if hasattr(x, 'wkt') else str(x))\n"
     ]
    }
   ],
   "source": [
    "# Select only the required columns for the database\n",
    "target_columns = [\n",
    "    'id', 'district_id', 'center_name', 'address', 'postal_code', \n",
    "    'latitude', 'longitude', 'geometry', 'neighborhood', \n",
    "    'district', 'neighborhood_id'\n",
    "]\n",
    "df_final = df_final[target_columns].copy()\n",
    "df_final['data_source'] = 'OSM_LOR'\n",
    "\n",
    "# Convert map objects to text (WKT) so AWS can accept the data\n",
    "df_final['geometry'] = df_final['geometry'].apply(lambda x: x.wkt if hasattr(x, 'wkt') else str(x))\n",
    "\n",
    "# SAVE TO CSV \n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "df_final.to_csv(\"output/jobcenters_berlin_final.csv\", index=False)\n",
    "print(f\"Local Backup: Saved {len(df_final)} records to output/jobcenters_berlin_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521bed4",
   "metadata": {},
   "source": [
    "# 6 DATABASE DEPLOYMENT & AWS Synchronization \n",
    "## 6.1 Database Infrastructure Setup\n",
    "In this stage intializing the connection to the AWS RDS (Relational Database Service). We use a secure tunnel to bridge the local development environment with the cloud infrastructure.\n",
    "\n",
    "Security Note: All credentials and session paths are sanitized before repository submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1467bfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/pandas/io/sql.py:2059: SAWarning: Did not recognize type 'geometry' of column 'geometry'\n",
      "  self.meta.reflect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: 63 records deployed with clean ID formatting.\n"
     ]
    }
   ],
   "source": [
    "# Use 'replace' to clear and fix the table structure\n",
    "try:\n",
    "    df_final.to_sql(\n",
    "        name='job_centers',\n",
    "        con=engine,\n",
    "        schema='berlin_source_data',\n",
    "        if_exists='replace', \n",
    "        index=False\n",
    "    )\n",
    "    print(f\"SUCCESS: {len(df_final)} records deployed with clean ID formatting.\")\n",
    "except Exception as e:\n",
    "    print(f\" Deployment failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc7b732",
   "metadata": {},
   "source": [
    "### Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4971f93",
   "metadata": {},
   "source": [
    "### Database Engine Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "66a55b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use generic placeholders for security before pushing to GitHub\n",
    "user_name = '' \n",
    "password = ''      \n",
    "host = '' \n",
    "port = ''\n",
    "database = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ec979e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'postgresql+psycopg2://{user_name}:{password}@{host}:{port}/{database}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fcde7a",
   "metadata": {},
   "source": [
    "## 6.2 Schema Definition & Relational \n",
    "Defining the table structure using explicit SQL DDL. To ensure the highest level of data quality implemented \n",
    "\n",
    "- Primary Key (id) - Ensures every job center has a unique, stable identifier.\n",
    "\n",
    "- Foreign Key (district_id) - Enforces referential integrity by linking our data to the official berlin_source_data.districts table.\n",
    "\n",
    "- Geospatial Serialization - Coordinates are stored as WKT (Well-Known Text) to ensure compatibility between Python's Shapely library and the PostgreSQL database driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fa79151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DDL Definition \n",
    "# 'district_id' for the reference as it's the standard for this database\n",
    "\n",
    "create_table_query = \"\"\"\n",
    "DROP TABLE IF EXISTS berlin_source_data.job_centers CASCADE;\n",
    "\n",
    "CREATE TABLE berlin_source_data.job_centers (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    district_id TEXT NOT NULL,\n",
    "    center_name TEXT,\n",
    "    address TEXT,\n",
    "    postal_code TEXT,\n",
    "    latitude DOUBLE PRECISION,\n",
    "    longitude DOUBLE PRECISION,\n",
    "    geometry TEXT,\n",
    "    neighborhood TEXT,\n",
    "    district TEXT,\n",
    "    neighborhood_id TEXT,\n",
    "    data_source TEXT,\n",
    "    CONSTRAINT fk_district FOREIGN KEY (district_id) \n",
    "        REFERENCES berlin_source_data.districts (district_id) -- Matching the LOR standard\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12254aba",
   "metadata": {},
   "source": [
    "##  6.3 Execute Table Creation(Data Sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "53a4abbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Schema Initialized.\n"
     ]
    }
   ],
   "source": [
    "# 2. Execute Table Creation\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table_query))\n",
    "    conn.commit()\n",
    "    print(\"Database Schema Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e0105cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Success 63 records successfully deployed to AWS Production.\n"
     ]
    }
   ],
   "source": [
    "#  3. Final Production Upload\n",
    "#  I use 'append' because the table was freshly created in the previous step\n",
    "df_final.to_sql(\n",
    "    name='job_centers',\n",
    "    con=engine,\n",
    "    schema='berlin_source_data',\n",
    "    if_exists='append', \n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\" Success {len(df_final)} records successfully deployed to AWS Production.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
